{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIATAR Dataset update\n",
    "\n",
    "Scripts can be run directly individually (sequentially), or using this notebook as a more interactive interface.\n",
    "\n",
    "The dataset is meant to be updated...\n",
    "1. periodically when data sources (SInAS, EPPO, DAISIE) publish new lists of species\n",
    "2. regularly (e.g., monthly, or 6-monthly) to incorporate new invasive records from EPPO and GBIF\n",
    "\n",
    "The update process can also be used to rebuild the dataset from scratch, though this will take several days to process many API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate from the tutorials folder to the root folder\n",
    "# Add that directory to the path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"..\")\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly (or multi-monthly) update\n",
    "\n",
    "This process updates only the occurrence data/first records, not the underlying species lists. This is a good way to incorporate new reports and species observations of ongoing invasions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run: 2_new_gbif_obs.py to get GBIF observations since the last update\n",
    "\n",
    "!python data_update/2_new_gbif_obs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run: 3b_get_monthly_eppo_reports.py to get EPPO reports since the last update\n",
    "\n",
    "!python data_update/3b_get_monthly_eppo_reports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run: 4_consolidate_all_occurence.py to incorporate these new records into the all_records and first_records datasets\n",
    "\n",
    "!python data_update/4_consolidate_all_occurence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full update/re-create dataset\n",
    "The same process is used to update the full dataset, including species lists and trait data, as is to create it from scratch. \n",
    "\n",
    "To update the species lists, first save the species list file from the source as described, then run the script. If there are no updates to a source, just run the script. This will prevent species that are already included in the dataset from being treated as new species. \n",
    "\n",
    "### Create .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create .env file. \n",
    "# If this is the first time you are running these scripts/creating the dataset, you will need to create an .env file. \n",
    "\n",
    "# Where the .csv files are being stored (data_dir)\n",
    "\n",
    "drive_letter = \"Y:\" \n",
    "data_dir = \"/GIATAR/dataset/\"\n",
    "\n",
    "# Auth token for EPPO API\n",
    " # Anyone can register on EPPO (https://data.eppo.int/user/login) to get a token\n",
    " \n",
    "eppo_token = \"INSERT_TOKEN\" \n",
    "\n",
    "# Year to start collecting GBIF records\n",
    "\n",
    "base_obs_year = 1970\n",
    "\n",
    "# Store information about last updates\n",
    "\n",
    "gbif_obs_last_update = \"2025-05-14\"\n",
    "eppo_report_last_update = \"2025-05-14\"\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    f.write(f\"DATA_PATH='{drive_letter + data_dir}'\\n\")\n",
    "    f.write(f\"EPPO_TOKEN='{eppo_token}'\\n\")\n",
    "    f.write(f\"BASE_OBS_YEAR='{base_obs_year}'\\n\")\n",
    "    f.write(f\"GBIF_OBS_UPDATED='{gbif_obs_last_update}'\\n\")\n",
    "    f.write(f\"EPPO_REP_UPDATED='{eppo_report_last_update}'\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download GBIF taxnomic backbone\n",
    "\n",
    "To create the dataset for the first time, you will also need to download the GBIF taxonomic backbone. \n",
    "\n",
    "Go to https://www.gbif.org/occurrence/download and select the Download tab. Select “Species List” (the last option). \n",
    "\n",
    "You should get an email notification when your download is available. Save file as `species lists/by_database/gbif_all_small.csv`.\n",
    "\n",
    "### Download species lists from each source\n",
    "\n",
    "- Download the latest SInAS list and records from https://zenodo.org/records/10038256 (if available) and save as `species lists/by_databaseSInAS_AlienSpeciesDB_2.5_FullTaxaList.csv` and `species lists/by_database/SInAS_AlienSpeciesDB_2.5.csv`\n",
    "- Download the CABI-ISC species list from: https://www.cabidigitallibrary.org/journal/cabicompendium/isdt#. Select and unselect a filter option to display full list. Download as CSV and save to `species lists/by_database/ISCSearchResults.csv`. Remove any headers and make sure columns are named \"Scientific name\", \"Common name\", \"Coverage\", \"URL\"\n",
    "- Download the Bayer flat file from the EPPO data services dashboard, https://data.eppo.int/user/ (see Bayer flat file: https://data.eppo.int/documentation/bayer). Save all files to `species lists/by_database/EPPO-main/`\n",
    "- Download the latest version of input_taxon.csv and save to `species lists/by_database/input_taxon.csv` from https://github.com/trias-project/daisie-checklist/tree/master/data/raw.\n",
    "\n",
    "### Run all of the species list scripts \n",
    "\n",
    "Regardless of whether new data is available for each source, run the scripts for all sources to prevent existing species from being treated as new species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/0b_get_sinas_species_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/0c_get_cabi_species_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/0d_get_eppo_species_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/0e_get_daisie_species_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all species for matches in the GBIF\n",
    "\n",
    "This step checks all of the species lists against the GBIF taxonomic backbone using GBIF's names API. This step can take some time depending on how many species are being searched for (minutes to hours for updating, ~10 hours to match the full original species lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/1a_new_species_gbif_match.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are unmatched species, run 1a2_check_unfound_gbif_keys.py\n",
    "\n",
    "! python data_update/1a2_check_unfound_gbif_keys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the invasive/alien species status of species in each list\n",
    "\n",
    "Some sources (EPPO, CABI) contain non-invasive and non-alien species (i.e., host species, natural enemies). This script checks the invasive species status against information from all four sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/1b_new_species_check_invasive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate same-species across lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/1c_combine_species_lists.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire species records\n",
    "\n",
    "Obtain and consolidate first reports and species occurrence records from GBIF, EPPO, DAISIE, ASFR, and CABI.\n",
    "\n",
    "- Get GBIF species observations: This step involves many API calls so may take several hours when updating the dataset (with more recent records or new species) and several days (~1 week) for the first construction of the dataset.\n",
    "- Get EPPO species distributions and species reports: This step involves web \n",
    "- Process the DAISIE data (formatting)\n",
    "- Consolidate the data from the different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/2_new_gbif_obs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/3a_get_eppo_species_report.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/3c_get_eppo_species_dist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all files in the DAISIE Github repository \"raw\" directory: https://github.com/trias-project/daisie-checklist/tree/master/data/raw and save to DAISIE data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/3d_process_daisie_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/4_consolidate_all_occurence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get additional traits data from EPPO\n",
    "\n",
    "CABI trait data is provided with the dataset as a static data set.\n",
    "\n",
    "Trait data from EPPO can be obtained via an API so can therefore be updated easily for new species or as new information about species becomes available. This is run only for new species when updating the dataset, or for all species when re-creating the dataset. Querying this data for all species can take 6+ hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_update/5_eppo_api_update.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You should now have a complete or updated dataset. Please let us know if you run into any issues or questions: https://github.com/ncsu-landscape-dynamics/GIATAR-dataset/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
